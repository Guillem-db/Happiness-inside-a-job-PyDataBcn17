{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(comments_path='data/anon_comments.csv',\n",
    "              inters_path='data/commentInteractions.csv',\n",
    "              lastpart_path='data/lastParticipationExists.csv',\n",
    "              votes_path='data/votes.csv'):\n",
    "    comments = comments = pd.read_csv(comments_path,parse_dates=[-1]).drop('Unnamed: 0',axis=1)\n",
    "    inters = pd.read_csv(inters_path).dropna(how='any').drop_duplicates()\n",
    "    lastpart = pd.read_csv(lastpart_path,parse_dates=[3]).drop_duplicates()\n",
    "    votes = pd.read_csv(votes_path,parse_dates=[2]).drop_duplicates()\n",
    "    return comments,inters,lastpart,votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_users_index(prediction_date,data_collection_ended,comments,votes,lastpart):\n",
    "\n",
    "    #get first dates on different DataFrames\n",
    "    min_date_comment = pd.DataFrame(comments.groupby(['companyAlias','employee'])['commentDate'].apply(np.min))\n",
    "    min_date_votes = pd.DataFrame(votes.groupby(['companyAlias','employee'])['voteDate'].apply(np.min))\n",
    "    min_date_part = pd.DataFrame(lastpart.groupby(['companyAlias','employee'])['lastParticipationDate'].min())\n",
    "    #combine them\n",
    "    min_dates = pd.merge(min_date_comment,min_date_votes,right_index=True,left_index=True,how='outer')\n",
    "    min_dates = pd.merge(min_dates,min_date_part,right_index=True,left_index=True,how='outer')\n",
    "    #take the earliest date of the 3 different DataFrames and propagate nans employee wise\n",
    "    first_dates = pd.DataFrame(min_dates.fillna(method='ffill',axis=1).fillna(method='bfill',axis=1).min(axis=1),\n",
    "                               columns=['first_date'])\n",
    "    last_dates = lastpart.set_index(['companyAlias','employee'])[['lastParticipationDate','stillExists']].copy()\n",
    "    dates = pd.merge(first_dates,last_dates,right_index=True,left_index=True,how='outer')\n",
    "    #take only users in date range\n",
    "    c_first = dates['first_date']<=pd.to_datetime(prediction_date)\n",
    "    c_last = dates['lastParticipationDate']>pd.to_datetime(prediction_date)#canvi\n",
    "    users_index = dates[c_first&c_last].copy()\n",
    "    return users_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def add_UIDs(users_index,votes,inters,comments,lastpart):\n",
    "    alias_comp = dict([(x,i) for i,x in enumerate(np.unique(users_index.index.levels[0].values).tolist())])\n",
    "    def make_uid(x):\n",
    "        df = x.reset_index().copy()\n",
    "        return df['companyAlias'].map(lambda x: str(alias_comp[x])).values+'_'+df['employee'].map(str).values\n",
    "\n",
    "    inters['uid'] = make_uid(inters)\n",
    "    inters['comid'] = inters['uid'].map(lambda x: x.split('_')[0]).values\n",
    "    votes['uid'] = make_uid(votes)\n",
    "    votes['comid'] = votes['uid'].map(lambda x: x.split('_')[0]).values\n",
    "    comments['uid'] = make_uid(comments)\n",
    "    comments['comid'] = comments['uid'].map(lambda x: x.split('_')[0]).values\n",
    "    lastpart['uid'] = make_uid(lastpart)\n",
    "    lastpart['comid'] = lastpart['uid'].map(lambda x: x.split('_')[0]).values\n",
    "    users_index['uid'] = make_uid(users_index)\n",
    "    users_index['comid'] = users_index['uid'].map(lambda x: x.split('_')[0]).values\n",
    "\n",
    "    inv_comp = dict([(val,key) for key,val in alias_comp.items()])\n",
    "    return users_index, votes, inters, comments, lastpart, inv_comp\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_by_uids(votes,comments,lastpart,inters, valid_employees):\n",
    "    filter_users = lambda x: x['uid'] in valid_employees\n",
    "    #create index of valid employees\n",
    "    coms_ix = comments.apply(filter_users,axis=1)\n",
    "    lastp_ix = lastpart.apply(filter_users,axis=1)\n",
    "    votes_ix = votes.apply(filter_users,axis=1)\n",
    "    ints_ix = inters.apply(filter_users,axis=1)\n",
    "    inters_clean = inters[ints_ix].copy()\n",
    "    votes_clean = votes[votes_ix].copy()\n",
    "    coms_clean = comments[coms_ix].copy()\n",
    "    lastp_clean = lastpart[lastp_ix].copy()\n",
    "    return votes_clean, coms_clean, lastp_clean, inters_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_interactions(votes,\n",
    "                        comments,\n",
    "                        inters,\n",
    "                        prediction_date,\n",
    "                        data_collection_ended):\n",
    "\n",
    "    #True before prediction date\n",
    "    target_votes_prev = votes['voteDate']<=pd.to_datetime(prediction_date)\n",
    "    valid_comments = comments[comments.commentDate<=pd.to_datetime(prediction_date)].commentId.unique()\n",
    "    #True before prediction date\n",
    "    target_votes_start = votes['voteDate']>pd.to_datetime(prediction_date)\n",
    "    #True before data collection ended\n",
    "    target_votes_end = votes['voteDate']<=pd.to_datetime(data_collection_ended)\n",
    "    \n",
    "    #num votes before prediction date\n",
    "    num_votes_pre = votes[target_votes_prev].groupby(['uid'])['vote'].agg({'votes_num': lambda x: len(x)}).copy().fillna(0)\n",
    "    #num votes after prediction date\n",
    "    num_votes_target = votes[target_votes_start&target_votes_end].groupby(['uid'])['vote'].agg({'votes_num': lambda x: len(x)}).copy().fillna(0)\n",
    "    #total number of likes given\n",
    "    num_likes = inters.groupby('uid')['liked'].sum().astype(int)\n",
    "    num_dislikes = inters.groupby('uid')['disliked'].sum().astype(int)\n",
    "    #voted at least 5 times before prediction date\n",
    "    pre_emp = num_votes_pre[num_votes_pre.values>=5].index.unique().values.tolist()\n",
    "    #voted at least 5 times after prediction date\n",
    "    post_emp = num_votes_target[num_votes_target.values>=2].index.unique().values.tolist()\n",
    "    #at least 5 likes/dislikes\n",
    "    enough_likes = num_likes[(num_likes.values+num_dislikes.values)>=5].index.unique().values.tolist()\n",
    "    valid_employees = [e for e in pre_emp if e in post_emp and e in enough_likes]\n",
    "    return valid_employees\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_training_set(votes_ml, comments_ml, inters_ml, prediction_date, data_collection_ended):\n",
    "    #votes training set\n",
    "    votes_train = votes_ml[votes_ml['voteDate']<=pd.to_datetime(prediction_date)]\n",
    "    #comments training set\n",
    "    comments_ml = comments_ml[comments_ml.commentDate<=pd.to_datetime(prediction_date)].copy()\n",
    "    valid_comments = comments_ml.commentId.unique().tolist()\n",
    "    #only interactions belonging to comments posted during the observation period\n",
    "    inters_ml = inters_ml[inters_ml['commentId'].map(lambda x: x in valid_comments)].copy()\n",
    "    \n",
    "    \n",
    "    target_votes_start = votes_ml['voteDate']>pd.to_datetime(prediction_date)\n",
    "    #True before data collection ended\n",
    "    target_votes_end = votes_ml['voteDate']<=pd.to_datetime(data_collection_ended)\n",
    "\n",
    "    target_happy = votes_ml[target_votes_start&target_votes_end].groupby(['uid'])['vote'].agg({'votes_num': lambda x: len(x)}).copy().fillna(0)\n",
    "\n",
    "    target_happy = pd.DataFrame(target_happy.votes_num.map(lambda x: 0 if x<=2 else 1))\n",
    "    return votes_train, comments_ml, inters_ml, target_happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_dataset(comments_path='data/anon_comments.csv',\n",
    "                    inters_path='data/commentInteractions.csv',\n",
    "                    lastpart_path='data/lastParticipationExists.csv',\n",
    "                    votes_path='data/votes.csv',\n",
    "                    prediction_date='6 Feb 2017',\n",
    "                    data_collection_ended='19 Feb 2017',\n",
    "                    suffix='',\n",
    "                    folder='clean_data' ):\n",
    "    print('Loading data...')\n",
    "    comments,inters,lastpart,votes = load_data(comments_path=comments_path,\n",
    "                                               inters_path=inters_path,\n",
    "                                               lastpart_path=lastpart_path,\n",
    "                                               votes_path=votes_path)\n",
    "    print(\"Filtering by valid UID\")\n",
    "    users_index = create_users_index(prediction_date,\n",
    "                                     data_collection_ended,\n",
    "                                     comments,\n",
    "                                     votes,\n",
    "                                     lastpart)\n",
    "    users_index, votes, inters, comments, lastpart, inv_comp = add_UIDs(users_index,\n",
    "                                                                        votes,\n",
    "                                                                        inters,\n",
    "                                                                        comments,\n",
    "                                                                        lastpart)\n",
    "    \n",
    "    valid_employees = users_index.uid.unique()\n",
    "    \n",
    "    votes, comments, lastpart, inters = filter_by_uids(votes,\n",
    "                                                       comments,\n",
    "                                                       lastpart,\n",
    "                                                       inters,\n",
    "                                                       valid_employees)\n",
    "    print(\"Filtering interactions...\")\n",
    "    valid_employees = filter_interactions(votes,\n",
    "                                          comments,\n",
    "                                          inters,\n",
    "                                          prediction_date,\n",
    "                                          data_collection_ended)\n",
    "    \n",
    "    votes_ml, comments_ml, lastpart_ml, inters_ml = filter_by_uids(votes,\n",
    "                                                                   comments,\n",
    "                                                                   lastpart,\n",
    "                                                                   inters,\n",
    "                                                                   valid_employees)\n",
    "    print(\"Preparing traing and test sets\")\n",
    "    votes_train, comments_train, inters_train, target_happy = filter_training_set(votes_ml,\n",
    "                                                                                  comments_ml,\n",
    "                                                                                  inters_ml,\n",
    "                                                                                  prediction_date,\n",
    "                                                                                  data_collection_ended)\n",
    "    print(\"Saving data...\")\n",
    "        \n",
    "    votes_train.to_csv(folder+'/votes_ml_'+suffix+'.csv')\n",
    "    comments_train.to_csv(folder+'/comments_ml_'+suffix+'.csv')\n",
    "    inters_train.to_csv(folder+'/interactions_ml_'+suffix+'.csv')\n",
    "    target_happy.to_csv(folder+'/target_ml_'+suffix+'.csv')\n",
    "    \n",
    "    print(\"Data is cleaned\")\n",
    "    return votes_train, comments_train, inters_train, target_happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalidus/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2881: DtypeWarning: Columns (2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering by valid UID\n",
      "Filtering interactions...\n",
      "Preparing traing and test sets\n",
      "Saving data...\n",
      "Data is cleaned\n"
     ]
    }
   ],
   "source": [
    "extract_dataset(prediction_date='9 Jan 2017',\n",
    "                data_collection_ended='22 Jan 2017',suffix='jan17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def employee_features(votes,comments,target):\n",
    "    vote_feats = votes.groupby(['uid'])['vote'].agg({'votes_1': lambda x: len(x[x==1]),\n",
    "                                                            'votes_2': lambda x: len(x[x==2]),\n",
    "                                                            'votes_3': lambda x: len(x[x==3]),\n",
    "                                                            'votes_4': lambda x: len(x[x==4]),\n",
    "                                                            'votes_mean': lambda x: x.mean(),\n",
    "                                                            'votes_std': lambda x: x.std(),\n",
    "                                                           'votes_num': lambda x: len(x),\n",
    "                                                           }).copy().fillna(0)\n",
    "    likes_feats = comments.groupby(['uid'])['likes'].agg({\n",
    "                                              'likes_num': lambda x: len(x),\n",
    "                                              'likes_mean': lambda x: x.mean(),\n",
    "                                              'likes_std': lambda x: x.std(),\n",
    "                                              'likes_sum': lambda x: x.sum(),\n",
    "                                                           })\n",
    "    dislikes_feats = comments.groupby(['uid'])['dislikes'].agg({\n",
    "                                              'dislikes_num': lambda x: len(x),\n",
    "                                              'dislikes_mean': lambda x: x.mean(),\n",
    "                                              'dislikes_std': lambda x: x.std(),\n",
    "                                              'dislikes_sum': lambda x: x.sum(),\n",
    "                                                           })\n",
    "    coms_feats = comments.dropna().groupby(['uid'])['comment'].agg({'com_num': lambda x: len(x),\n",
    "                                                                 'com_mean': lambda x: x.apply(len).mean(),\n",
    "                                                                 'com_std': lambda x: x.map(len).std(),\n",
    "                                                                 'com_sum': lambda x: x.map(len).sum()})\n",
    "    features = pd.merge(vote_feats,likes_feats,left_index=True,right_index=True,how='outer')\n",
    "    features = pd.merge(features,dislikes_feats,left_index=True,right_index=True,how='outer')\n",
    "    features = pd.merge(features,coms_feats,left_index=True,right_index=True,how='outer')\n",
    "    features = pd.merge(features,target,left_index=True,right_on='uid',how='right')#fill users with 0 comments\n",
    "    E_features = features.copy()\n",
    "    E_features.rename(columns=dict([(x,'E_'+str(x)) for x in E_features.columns]),inplace=True)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def link_all(users_index,comments,inter):\n",
    "    \"\"\"Returns a graph containing relationships of like/dislike among employees\"\"\"\n",
    "    g = nx.DiGraph()\n",
    "    #One node for every employee storing its employee and company ids\n",
    "    for ix,x in users_index.iterrows():\n",
    "        if not g.has_node(x['uid']):\n",
    "            c,u = x['uid'].split('_')\n",
    "            g.add_node(x['uid'],company=c,employee=u)\n",
    "    #adding links to a graph based on likes/dislikes\n",
    "    com_ids = comments['commentId'].unique()\n",
    "    users = comments['uid'].unique().tolist()\n",
    "    print(\"linking {} comments from {} users\".format(len(com_ids),len(users)))\n",
    "    for com in com_ids:\n",
    "        #who frite current comment\n",
    "        writer = comments[comments['commentId']==com]['uid'].values[0]\n",
    "        #info about com\n",
    "        df = inter[inter['commentId']==com]\n",
    "        #people who disliked the comment\n",
    "        haters = df[df['liked']==0]['uid'].values.copy().tolist()\n",
    "        #people who like the comment\n",
    "        likers = df[df['liked']==1]['uid'].values.copy().tolist()\n",
    "        for i,u in enumerate(likers):\n",
    "            if not g.has_edge(u,writer):\n",
    "                g.add_edge(u,writer,int_sum=1,interactions=1,liked=1,disliked=0)\n",
    "            else:\n",
    "                g.edge[u][writer]['interactions'] += 1\n",
    "                g.edge[u][writer]['int_sum'] += 1\n",
    "                g.edge[u][writer]['liked'] += 1\n",
    "\n",
    "        for i,u in enumerate(haters):\n",
    "            if not g.has_edge(u,writer):\n",
    "                g.add_edge(u,writer,int_sum=-1,interactions=1,liked=0,disliked=1)\n",
    "            else:\n",
    "                g.edge[u][writer]['interactions'] += 1\n",
    "                g.edge[u][writer]['disliked'] += 1\n",
    "                g.edge[u][writer]['int_sum'] -= 1\n",
    "    return g     \n",
    "\n",
    "def add_info_to_graph(features,G):\n",
    "    \"\"\"Adds the information contained in a DataFrame to a networkx graph\"\"\"\n",
    "    for n in G.nodes_iter():\n",
    "        fs = features.ix[n].to_dict()\n",
    "        G.add_node(n,**fs)\n",
    "    return G\n",
    "\n",
    "def add_rel_likes_metrics(g):\n",
    "    \"\"\"Adds infomation to edges about the relative number of likes a given employee gave to another.\"\"\"\n",
    "    for src in g.nodes_iter():\n",
    "        t_rel = 0\n",
    "        neigh = nx.neighbors(g,src)\n",
    "        f = 1 if len(neigh)==0 else len(neigh)\n",
    "        for dst in neigh:\n",
    "            rel = g.edge[src][dst]['liked']/g.edge[src][dst]['interactions']\n",
    "            g.edge[src][dst]['rel_agree'] = rel\n",
    "            t_rel += rel\n",
    "        g.node[src]['mean_agree'] = t_rel/f\n",
    "\n",
    "def calculate_metrics(g,metrics,weight=None):\n",
    "    \"\"\"Returns a DataFrame containing the metrics of a given graph.\n",
    "    It will calculate the metrics both in the directed and  undirected version opf the graph\"\"\"\n",
    "    graph_df = pd.DataFrame()\n",
    "    nw =  '' if weight is None else weight+'_'\n",
    "    for met in metrics:\n",
    "        graph_df['G_'+nw+met.__name__] = pd.Series(met(g))\n",
    "        graph_df['G_'+nw+'w_'+met.__name__] = pd.Series(met(g,weight=weight))\n",
    "        graph_df['G_'+nw+met.__name__+'_u'] = pd.Series(met(g.to_undirected()))\n",
    "        graph_df['G_'+nw+'w_'+met.__name__+'_u'] = pd.Series(met(g.to_undirected(),weight=weight))\n",
    "    #Normalize metrics from 0 to 1\n",
    "    graph_df = (graph_df-graph_df.min(axis=0))/(graph_df.max(axis=0)-graph_df.min(axis=0))\n",
    "    return graph_df\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "def get_clusters(g, weight=None,n_components=2,companyAlias=''):\n",
    "    \"\"\"Return a DataFrame containing the NMF clustering information of a given graph\"\"\"\n",
    "    nw =  '' if weight is None else weight+'_'\n",
    "    #Calculate on directed version of the graph\n",
    "    model = NMF(n_components=n_components)\n",
    "    X = nx.adjacency_matrix(g,weight=weight).todense()\n",
    "    if (X < 0).any():\n",
    "        X = np.abs(X)#NMF only accepts non-negative values\n",
    "    communities = model.fit_transform(X)\n",
    "    cd = pd.DataFrame(index=list(g.nodes_iter()),\n",
    "                      columns=['G_'+nw+'NMF'+str(i)+'_d' for i in range(1,n_components+1)],\n",
    "                      data=communities) \n",
    "    #Undirected version\n",
    "    Xd = nx.adjacency_matrix(g.to_undirected(),weight=weight).todense()\n",
    "    if (Xd < 0).any():\n",
    "        Xd = np.abs(Xd)\n",
    "    communities = model.fit_transform(Xd)\n",
    "    clusters = cd.combine_first(pd.DataFrame(index=cd.index,\n",
    "                                             columns=['G_'+nw+'NMF'+str(i)+'_u' for i in range(1,n_components+1)]\n",
    "                                             ,data=communities))\n",
    "    return clusters \n",
    "\n",
    "def calculate_graph_features(G,features):\n",
    "    add_rel_likes_metrics(G)\n",
    "    ng = add_info_to_graph(features=features,G=G.copy())\n",
    "    \n",
    "    met_funcs = [nx.degree, nx.betweenness_centrality]\n",
    "    graph_met_rel = calculate_metrics(ng,met_funcs,weight='rel_agree')\n",
    "    cluster_rel = get_clusters(ng,weight='rel_agree')\n",
    "    agree_feats = pd.merge(graph_met_rel,cluster_rel,left_index=True,right_index=True,how='outer')\n",
    "    graph_met_int = calculate_metrics(ng,met_funcs,weight='interactions')\n",
    "    cluster_int = get_clusters(ng,weight='interactions')\n",
    "    int_feats = pd.merge(graph_met_int,cluster_int,left_index=True,right_index=True,how='outer')\n",
    "    graph_feats = pd.merge(int_feats,agree_feats,left_index=True,right_index=True,how='outer')\n",
    "    #final_features = pd.merge(features,graph_feats,left_index=True,right_index=True,how='outer')\n",
    "    ng = add_info_to_graph(features=graph_feats,G=ng.copy())\n",
    "    \n",
    "    return graph_feats,ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def emp_graph_features(target, comments, inters, e_features):\n",
    "    \n",
    "    G_raw = link_all(target,comments,inters)\n",
    "    EG_features,g_total = calculate_graph_features(G_raw.copy(),e_features.set_index('uid'))\n",
    "    return EG_features,g_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def company_features(votes, comments,target):\n",
    "    comp_vote_feats = votes.groupby(['comid'])['vote'].agg({'votes_1': lambda x: len(x[x==1]),\n",
    "                                                        'votes_2': lambda x: len(x[x==2]),\n",
    "                                                        'votes_3': lambda x: len(x[x==3]),\n",
    "                                                        'votes_4': lambda x: len(x[x==4]),\n",
    "                                                        'votes_mean': lambda x: x.mean(),\n",
    "                                                        'votes_std': lambda x: x.std(),\n",
    "                                                       'votes_num': lambda x: len(x),\n",
    "                                                       }).copy()\n",
    "    comp_likes_feats = comments.groupby(['comid'])['likes'].agg({\n",
    "                                          'likes_num': lambda x: len(x),\n",
    "                                          'likes_mean': lambda x: x.mean(),\n",
    "                                          'likes_std': lambda x: x.std(),\n",
    "                                          'likes_sum': lambda x: x.sum(),\n",
    "                                                       })\n",
    "    comp_dislikes_feats = comments.groupby(['comid'])['dislikes'].agg({\n",
    "                                              'dislikes_num': lambda x: len(x),\n",
    "                                              'dislikes_mean': lambda x: x.mean(),\n",
    "                                              'dislikes_std': lambda x: x.std(),\n",
    "                                              'dislikes_sum': lambda x: x.sum(),\n",
    "                                                           })\n",
    "    comp_coms_feats = comments.dropna().groupby(['comid'])['comment'].agg({'com_num': lambda x: len(x),\n",
    "                                                                 'com_mean': lambda x: x.apply(len).mean(),\n",
    "                                                                 'com_std': lambda x: x.map(len).std(),\n",
    "                                                                 'com_sum': lambda x: x.map(len).sum()})\n",
    "    comp_features = pd.merge(comp_vote_feats,comp_likes_feats,left_index=True,right_index=True,how='outer')\n",
    "    comp_features = pd.merge(comp_features,comp_dislikes_feats,left_index=True,right_index=True,how='outer')\n",
    "    comp_features = pd.merge(comp_features,comp_coms_feats,left_index=True,right_index=True,how='outer')\n",
    "    comp_df = comp_features.fillna(1)\n",
    "    C_features_raw = comp_df.applymap(lambda x: 1 if x==0 else x)\n",
    "    C_features = pd.DataFrame(index=target.uid,columns=C_features_raw.columns)\n",
    "    for i in range(len(target.uid)):\n",
    "        company = int(C_features.index.values[i].split('_')[0])\n",
    "        C_features.iloc[i,:] = C_features_raw.ix[company].values\n",
    "    return C_features, C_features_raw\n",
    "    \n",
    "def calculate_ce_features(C_features_raw, comments, inters, target, features):\n",
    "    #features['comid'] = features['uid'].map(lambda x: x.split('_')[0] )\n",
    "    comments['comid'] = comments['uid'].map(lambda x: x.split('_')[0] )\n",
    "    inters['comid'] = inters['uid'].map(lambda x: x.split('_')[0] )\n",
    "    target['comid'] = target['uid'].map(lambda x: x.split('_')[0] )\n",
    "    rel_feats = features.set_index('uid')[C_features_raw.columns.values.tolist()+['comid']].copy()\n",
    "    companies = C_features_raw.index.values.tolist()\n",
    "    for comp in companies:\n",
    "        rel_feats.loc[rel_feats['comid']==int(comp)] = rel_feats.loc[rel_feats['comid']==int(comp)]/C_features_raw.ix[comp]\n",
    "    #rel_feats.drop('comid',axis=1,inplace=True)\n",
    "\n",
    "    rel_feats.rename(columns=dict([(x,'CE_'+str(x)) for x in rel_feats.columns]),inplace=True)\n",
    "\n",
    "    CE_features = rel_feats.copy().fillna(0.)\n",
    "    return CE_features,companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_company_graph_data(comp,target,comments,inters,features):\n",
    "    g_comps = {}\n",
    "    feats = pd.DataFrame()\n",
    "    comp = str(comp)\n",
    "    ui = target[target['comid']==comp].copy()\n",
    "    co = comments[comments['comid']==comp].copy()\n",
    "    ints = inters[inters['comid']==comp].copy()\n",
    "    g_c = link_all(ui,co,ints)\n",
    "    if g_c.node!={}:\n",
    "        g_comps[comp] = g_c\n",
    "        _features,g_c = calculate_graph_features(g_c,features[features['CE_comid'].astype(int)==int(comp)].copy())\n",
    "        feats = _features.copy()\n",
    "    return feats,g_comps\n",
    "\n",
    "def ce_graph_features(companies,target,comments,inters,ce_feats):\n",
    "    \n",
    "\n",
    "    rgdf,g_comps  = one_company_graph_data(companies[0],target,comments,inters,ce_feats)\n",
    "    for comp in companies[1:]:\n",
    "        print(\"calculating comp : {}\".format(comp))\n",
    "        _rgdf,_g_comps  = one_company_graph_data(comp,target,comments,inters,ce_feats)\n",
    "        g_comps.update(_g_comps)\n",
    "        rgdf = pd.concat([rgdf,_rgdf])\n",
    "    CEG_features = rgdf.rename(columns=dict([(x,'CE'+str(x)) for x in rgdf.columns])).copy()\n",
    "    return CEG_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_features(comments_path='clean_data/comments_ml.csv',\n",
    "                       inters_path='clean_data/interactions_ml.csv',\n",
    "                       target_path='clean_data/target_ml.csv',\n",
    "                       votes_path='clean_data/votes_ml.csv'):\n",
    "    comments = pd.read_csv(comments_path,parse_dates=[-3]).drop('Unnamed: 0',axis=1)\n",
    "    votes = pd.read_csv(votes_path,parse_dates=[-4]).drop('Unnamed: 0',axis=1)\n",
    "    inters = pd.read_csv(inters_path).drop('Unnamed: 0',axis=1)\n",
    "    target = pd.read_csv(target_path).rename(columns={'votes_num':'target'})\n",
    "    print(\"Calculating employee features\")\n",
    "    features = employee_features(votes,comments=comments,target=target)\n",
    "    features['comid'] = features['uid'].map(lambda x: x.split('_')[0] )\n",
    "    e_features = features.rename(columns=dict([(x,'E_'+str(x)) for x in features.columns]))\n",
    "\n",
    "    #return e_features\n",
    "    print(\"Calculating employee graph features\")\n",
    "    eg_features,g_total = emp_graph_features(target, comments, inters, features)\n",
    "    print(\"Calculating company features\")\n",
    "    c_features, c_features_raw = company_features(votes, comments, target)\n",
    "    print(\"Calculating company-employee features\")\n",
    "    ce_features,companies = calculate_ce_features(c_features_raw, comments, inters, target, features)\n",
    "    print(\"Calculating company-employee graph features\")\n",
    "    ceg_features = ce_graph_features(companies,target,comments,inters,ce_features)\n",
    "    return e_features,eg_features,c_features,ce_features,ceg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating employee features\n",
      "Calculating employee graph features\n",
      "Calculating company features\n",
      "Calculating company-employee features\n",
      "Calculating company-employee graph features\n",
      "linking 213 comments from 10 users\n",
      "calculating comp : 4\n",
      "linking 2762 comments from 87 users\n",
      "calculating comp : 6\n",
      "linking 1760 comments from 78 users\n",
      "calculating comp : 11\n",
      "linking 3062 comments from 130 users\n",
      "calculating comp : 12\n",
      "linking 905 comments from 27 users\n",
      "calculating comp : 13\n",
      "linking 474 comments from 14 users\n",
      "calculating comp : 16\n",
      "linking 1691 comments from 99 users\n",
      "calculating comp : 17\n",
      "linking 789 comments from 39 users\n",
      "calculating comp : 18\n",
      "linking 1224 comments from 86 users\n",
      "calculating comp : 19\n",
      "linking 233 comments from 7 users\n",
      "calculating comp : 20\n",
      "linking 608 comments from 36 users\n",
      "calculating comp : 21\n",
      "linking 765 comments from 56 users\n",
      "calculating comp : 22\n",
      "linking 487 comments from 23 users\n",
      "calculating comp : 23\n",
      "linking 488 comments from 23 users\n",
      "calculating comp : 24\n",
      "linking 339 comments from 24 users\n",
      "calculating comp : 25\n",
      "linking 18 comments from 1 users\n",
      "calculating comp : 26\n",
      "linking 712 comments from 67 users\n",
      "calculating comp : 29\n",
      "linking 150 comments from 23 users\n",
      "calculating comp : 30\n",
      "linking 449 comments from 21 users\n",
      "calculating comp : 31\n",
      "linking 790 comments from 49 users\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'eg_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-d41666a8ddfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-05b04d75f548>\u001b[0m in \u001b[0;36mcalculate_features\u001b[0;34m(comments_path, inters_path, target_path, votes_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Calculating company-employee graph features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mceg_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce_graph_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompanies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mce_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0me_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meg_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mce_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mceg_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'eg_features' is not defined"
     ]
    }
   ],
   "source": [
    "feats = calculate_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_dataset(prediction_date='9 Jan 2017',\n",
    "                    data_collection_ended='22 Jan 2017',\n",
    "                    suffix='jan17',\n",
    "                    folder='clean_data'\n",
    "                   ):\n",
    "    extract_dataset(prediction_date=prediction_date,\n",
    "                    data_collection_ended=data_collection_ended,\n",
    "                    suffix=suffix,\n",
    "                    folder=folder\n",
    "                   )\n",
    "    e_features, eg_features,c_features,ce_features,ceg_features = calculate_features(comments_path=folder+'/comments_ml_'+suffix+'.csv',\n",
    "                                                                                   inters_path=folder+'/interactions_ml_'+suffix+'.csv',\n",
    "                                                                                   target_path=folder+'/target_ml_'+suffix+'.csv',\n",
    "                                                                                   votes_path=folder+'/votes_ml_'+suffix+'.csv')\n",
    "    \n",
    "    e_features.to_csv('features/e_features'+suffix+'.csv')\n",
    "    eg_features.to_csv('features/eg_features'+suffix+'.csv')\n",
    "    c_features.to_csv('features/c_features'+suffix+'.csv')\n",
    "    ce_features.to_csv('features/ce_features'+suffix+'.csv')\n",
    "    ceg_features.to_csv('features/ceg_features'+suffix+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starts = ['14 Nov 2016','5 Dec 2016','19 Dec 2016']\n",
    "ends = ['4 Dec 2016','18 Dec 2016','1 Jan 2017']\n",
    "sufixes = ['nov14','dec5','dec19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalidus/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: DtypeWarning: Columns (2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering by valid UID\n",
      "Filtering interactions...\n",
      "Preparing traing and test sets\n",
      "Saving data...\n",
      "Data is cleaned\n",
      "Calculating employee features\n",
      "Calculating employee graph features\n",
      "linking 15628 comments from 1013 users\n",
      "Calculating company features\n",
      "Calculating company-employee features\n",
      "Calculating company-employee graph features\n",
      "linking 366 comments from 17 users\n",
      "calculating comp : 1\n",
      "linking 488 comments from 11 users\n",
      "calculating comp : 3\n",
      "linking 97 comments from 5 users\n",
      "calculating comp : 4\n",
      "linking 3168 comments from 102 users\n",
      "calculating comp : 6\n",
      "linking 1444 comments from 101 users\n",
      "calculating comp : 8\n",
      "linking 331 comments from 21 users\n",
      "calculating comp : 11\n",
      "linking 2580 comments from 155 users\n",
      "calculating comp : 12\n",
      "linking 931 comments from 35 users\n",
      "calculating comp : 13\n",
      "linking 370 comments from 16 users\n",
      "calculating comp : 16\n",
      "linking 1399 comments from 100 users\n",
      "calculating comp : 17\n",
      "linking 824 comments from 53 users\n",
      "calculating comp : 18\n",
      "linking 993 comments from 83 users\n",
      "calculating comp : 19\n",
      "linking 137 comments from 6 users\n",
      "calculating comp : 20\n",
      "linking 248 comments from 38 users\n",
      "calculating comp : 21\n",
      "linking 434 comments from 58 users\n",
      "calculating comp : 22\n",
      "linking 236 comments from 23 users\n",
      "calculating comp : 23\n",
      "linking 280 comments from 24 users\n",
      "calculating comp : 24\n",
      "linking 697 comments from 72 users\n",
      "calculating comp : 25\n",
      "linking 21 comments from 2 users\n",
      "calculating comp : 26\n",
      "linking 549 comments from 78 users\n",
      "calculating comp : 29\n",
      "linking 10 comments from 5 users\n",
      "calculating comp : 30\n",
      "linking 25 comments from 8 users\n",
      "Loading data...\n",
      "Filtering by valid UID\n",
      "Filtering interactions...\n",
      "Preparing traing and test sets\n",
      "Saving data...\n",
      "Data is cleaned\n",
      "Calculating employee features\n",
      "Calculating employee graph features\n",
      "linking 15254 comments from 893 users\n",
      "Calculating company features\n",
      "Calculating company-employee features\n",
      "Calculating company-employee graph features\n",
      "linking 154 comments from 8 users\n",
      "calculating comp : 3\n",
      "linking 91 comments from 4 users\n",
      "calculating comp : 4\n",
      "linking 3158 comments from 92 users\n",
      "calculating comp : 6\n",
      "linking 1556 comments from 97 users\n",
      "calculating comp : 11\n",
      "linking 2435 comments from 121 users\n",
      "calculating comp : 12\n",
      "linking 1006 comments from 33 users\n",
      "calculating comp : 13\n",
      "linking 448 comments from 18 users\n",
      "calculating comp : 16\n",
      "linking 1532 comments from 100 users\n",
      "calculating comp : 17\n",
      "linking 722 comments from 40 users\n",
      "calculating comp : 18\n",
      "linking 930 comments from 67 users\n",
      "calculating comp : 19\n",
      "linking 155 comments from 5 users\n",
      "calculating comp : 20\n",
      "linking 340 comments from 35 users\n",
      "calculating comp : 21\n",
      "linking 488 comments from 49 users\n",
      "calculating comp : 22\n",
      "linking 307 comments from 24 users\n",
      "calculating comp : 23\n",
      "linking 366 comments from 24 users\n",
      "calculating comp : 24\n",
      "linking 669 comments from 49 users\n",
      "calculating comp : 26\n",
      "linking 624 comments from 76 users\n",
      "calculating comp : 29\n",
      "linking 81 comments from 14 users\n",
      "calculating comp : 30\n",
      "linking 121 comments from 12 users\n",
      "calculating comp : 31\n",
      "linking 71 comments from 25 users\n",
      "Loading data...\n",
      "Filtering by valid UID\n",
      "Filtering interactions...\n",
      "Preparing traing and test sets\n",
      "Saving data...\n",
      "Data is cleaned\n",
      "Calculating employee features\n",
      "Calculating employee graph features\n",
      "linking 15338 comments from 854 users\n",
      "Calculating company features\n",
      "Calculating company-employee features\n",
      "Calculating company-employee graph features\n",
      "linking 180 comments from 12 users\n",
      "calculating comp : 3\n",
      "linking 33 comments from 1 users\n",
      "calculating comp : 4\n",
      "linking 3292 comments from 93 users\n",
      "calculating comp : 6\n",
      "linking 1655 comments from 89 users\n",
      "calculating comp : 11\n",
      "linking 2367 comments from 109 users\n",
      "calculating comp : 12\n",
      "linking 849 comments from 23 users\n",
      "calculating comp : 13\n",
      "linking 346 comments from 13 users\n",
      "calculating comp : 16\n",
      "linking 1621 comments from 100 users\n",
      "calculating comp : 17\n",
      "linking 619 comments from 31 users\n",
      "calculating comp : 18\n",
      "linking 936 comments from 65 users\n",
      "calculating comp : 19\n",
      "linking 159 comments from 5 users\n",
      "calculating comp : 20\n",
      "linking 471 comments from 39 users\n",
      "calculating comp : 21\n",
      "linking 515 comments from 48 users\n",
      "calculating comp : 22\n",
      "linking 361 comments from 22 users\n",
      "calculating comp : 23\n",
      "linking 285 comments from 15 users\n",
      "calculating comp : 24\n",
      "linking 391 comments from 36 users\n",
      "calculating comp : 26\n",
      "linking 678 comments from 77 users\n",
      "calculating comp : 29\n",
      "linking 86 comments from 10 users\n",
      "calculating comp : 30\n",
      "linking 162 comments from 13 users\n",
      "calculating comp : 31\n",
      "linking 332 comments from 53 users\n"
     ]
    }
   ],
   "source": [
    "for s,e,suf in zip(starts,ends,sufixes):\n",
    "    process_dataset(prediction_date=s, data_collection_ended=e,suffix=suf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_data/target_ml_'+sufixes[0]+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>votes_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0_32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0_33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0_37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0_38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0_39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0_51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0_52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0_55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0_56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0_57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0_62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0_63</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0_64</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11_100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11_101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11_102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>11_103</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>11_107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11_108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>11_114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>11_116</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11_119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11_12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>11_122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>6_8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>6_81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>6_87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>6_9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>6_92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>6_93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>6_95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>6_98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>8_101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>8_104</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>8_109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>8_113</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>8_117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>8_130</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>8_132</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>8_138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>8_144</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>8_159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>8_160</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>8_17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>8_211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>8_213</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>8_230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>8_232</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>8_236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>8_237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>8_42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>8_5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>8_81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>8_9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1220 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         uid  votes_num\n",
       "0       0_18          1\n",
       "1       0_20          1\n",
       "2      0_259          1\n",
       "3       0_27          1\n",
       "4       0_31          1\n",
       "5       0_32          1\n",
       "6       0_33          0\n",
       "7       0_37          1\n",
       "8       0_38          1\n",
       "9       0_39          1\n",
       "10      0_51          1\n",
       "11      0_52          1\n",
       "12      0_55          1\n",
       "13      0_56          1\n",
       "14      0_57          1\n",
       "15      0_62          1\n",
       "16      0_63          1\n",
       "17      0_64          1\n",
       "18     11_10          1\n",
       "19    11_100          1\n",
       "20    11_101          1\n",
       "21    11_102          1\n",
       "22    11_103          1\n",
       "23    11_107          1\n",
       "24    11_108          0\n",
       "25    11_114          1\n",
       "26    11_116          1\n",
       "27    11_119          1\n",
       "28     11_12          1\n",
       "29    11_122          1\n",
       "...      ...        ...\n",
       "1190     6_8          1\n",
       "1191    6_81          1\n",
       "1192    6_87          1\n",
       "1193     6_9          1\n",
       "1194    6_92          1\n",
       "1195    6_93          1\n",
       "1196    6_95          1\n",
       "1197    6_98          1\n",
       "1198   8_101          0\n",
       "1199   8_104          0\n",
       "1200   8_109          1\n",
       "1201   8_113          1\n",
       "1202   8_117          1\n",
       "1203   8_130          1\n",
       "1204   8_132          1\n",
       "1205   8_138          0\n",
       "1206   8_144          0\n",
       "1207   8_159          1\n",
       "1208   8_160          0\n",
       "1209    8_17          0\n",
       "1210   8_211          1\n",
       "1211   8_213          1\n",
       "1212   8_230          0\n",
       "1213   8_232          0\n",
       "1214   8_236          1\n",
       "1215   8_237          1\n",
       "1216    8_42          1\n",
       "1217     8_5          1\n",
       "1218    8_81          1\n",
       "1219     8_9          0\n",
       "\n",
       "[1220 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_datas(sufixes=['']):\n",
    "    e_features = pd.read_csv('features/e_features'+sufixes[0]+'.csv')\n",
    "    eg_features = pd.read_csv('features/eg_features'+sufixes[0]+'.csv')\n",
    "    c_features = pd.read_csv('features/c_features'+sufixes[0]+'.csv')\n",
    "    ce_features = pd.read_csv('features/ce_features'+sufixes[0]+'.csv')\n",
    "    ceg_features = pd.read_csv('features/ceg_features'+sufixes[0]+'.csv')\n",
    "    target = pd.read_csv('clean_data/target_ml_'+sufixes[0]+'.csv')\n",
    "    #return e_features,eg_features,c_features,ce_features,ceg_features\n",
    "    for suf in sufixes[1:]:\n",
    "        _target = pd.read_csv('clean_data/target_ml_'+suf+'.csv')\n",
    "        _target['uid'] = _target['uid']+'_'+suf\n",
    "        target = pd.concat([target,_target])\n",
    "        \n",
    "        \n",
    "        _e_features =  pd.read_csv('features/e_features'+suf+'.csv')\n",
    "        _e_features['E_uid'] = _e_features['E_uid']+'_'+suf\n",
    "        e_features = pd.concat([e_features,_e_features])\n",
    "        \n",
    "        _eg_features = pd.read_csv('features/eg_features'+suf+'.csv')\n",
    "        _eg_features['Unnamed: 0'] = _eg_features['Unnamed: 0']+'_'+suf\n",
    "        eg_features = pd.concat([eg_features,_eg_features])\n",
    "        \n",
    "        _c_features = pd.read_csv('features/c_features'+suf+'.csv')\n",
    "        _c_features['uid'] = _c_features['uid']+'_'+suf\n",
    "        c_features = pd.concat([c_features,_c_features])\n",
    "        \n",
    "        _ce_features = pd.read_csv('features/ce_features'+suf+'.csv')\n",
    "        _ce_features['uid'] = _ce_features['uid']+'_'+suf\n",
    "        ce_features = pd.concat([ce_features,_ce_features])\n",
    "        \n",
    "        _ceg_features = pd.read_csv('features/ceg_features'+suf+'.csv')\n",
    "        _ceg_features['Unnamed: 0'] = _ceg_features['Unnamed: 0']+'_'+suf\n",
    "        ceg_features = pd.concat([ceg_features,_ceg_features])\n",
    "    e_features.rename(columns={'E_uid':'uid'},inplace=True)\n",
    "    eg_features.rename(columns={'Unnamed: 0':'uid'},inplace=True)\n",
    "    #c_features.rename(columns={'E_uid':'uid'},inplace=True)\n",
    "    #ce_features.rename(columns={'E_uid':'uid'},inplace=True)\n",
    "    ceg_features.rename(columns={'Unnamed: 0':'uid'},inplace=True)\n",
    "    \n",
    "    features = pd.merge(e_features,eg_features,left_on='uid',right_on='uid')\n",
    "    features = pd.merge(features,c_features,left_on='uid',right_on='uid')\n",
    "    features = pd.merge(features,ce_features,left_on='uid',right_on='uid')\n",
    "    features = pd.merge(features,ceg_features,left_on='uid',right_on='uid')\n",
    "    return e_features,eg_features,c_features,ce_features,ceg_features,features,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datas = merge_datas(sufixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e_features,eg_features,c_features,ce_features,ceg_features,features,target = datas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "suffix = 'all'\n",
    "e_features.to_csv('features/e_features'+suffix+'.csv')\n",
    "eg_features.to_csv('features/eg_features'+suffix+'.csv')\n",
    "c_features.to_csv('features/c_features'+suffix+'.csv')\n",
    "ce_features.to_csv('features/ce_features'+suffix+'.csv')\n",
    "ceg_features.to_csv('features/ceg_features'+suffix+'.csv')\n",
    "features.to_csv('features/all_features'+suffix+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target.to_csv('features/target'+suffix+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
